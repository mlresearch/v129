---
title: Inverse Visual Question Answering with Multi-Level Attentions
crossref: acml20
abstract: Inverse Visual Question Answering (iVQA) is a contemporary task emerged
  from the need of improving visual and language understanding. It tackles the challenging
  problem of generating a corresponding question for a given image-answer pair. In
  this paper, we propose a novel deep multi-level attention model to address inverse
  visual question answering. The proposed model generates regional visual and semantic
  features at the object level and then enhances them with the answer cue by using
  attention mechanisms. Two levels of multiple attentions are employed in the model,
  including the dual attention at the partial question encoding step and the dynamic
  attention at the questionâ€™s next word generation step. We evaluate the proposed
  model on the VQA V1 dataset. It demonstrates the state-of-the-art performance in
  terms of multiple commonly used metrics.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: alwattar20a
month: 0
tex_title: Inverse Visual Question Answering with Multi-Level Attentions
firstpage: 449
lastpage: 464
page: 449-464
order: 449
cycles: false
bibtex_author: Alwattar, Yaser and Guo, Yuhong
author:
- given: Yaser
  family: Alwattar
- given: Yuhong
  family: Guo
date: 2020-09-25
address: 
container-title: Proceedings of The 12th Asian Conference on Machine Learning
volume: '129'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 25
pdf: http://proceedings.mlr.press/v129/alwatter20a/alwattar20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
