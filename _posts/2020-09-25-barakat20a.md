---
title: Convergence Rates of a Momentum Algorithm with Bounded Adaptive Step Size for
  Nonconvex Optimization
crossref: acml20
abstract: Although Adam is a very popular algorithm for optimizing the weights of
  neural networks, it has been recently shown that it can diverge even in simple convex
  optimization examples. Several variants of Adam have been proposed to circumvent
  this convergence issue. In this work, we study the Adam algorithm for smooth nonconvex
  optimization under a boundedness assumption on the adaptive learning rate. The bound
  on the adaptive step size depends on the Lipschitz constant of the gradient of the
  objective function and provides safe theoretical adaptive step sizes. Under this
  boundedness assumption, we show a novel first order convergence rate result in both
  deterministic and stochastic contexts. Furthermore, we establish convergence rates
  of the function value sequence using the Kurdyka-Lojasiewicz property.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: barakat20a
month: 0
tex_title: Convergence Rates of a Momentum Algorithm with Bounded Adaptive Step Size
  for Nonconvex Optimization
firstpage: 225
lastpage: 240
page: 225-240
order: 225
cycles: false
bibtex_author: Barakat, Anas and Bianchi, Pascal
author:
- given: Anas
  family: Barakat
- given: Pascal
  family: Bianchi
date: 2020-09-25
address: 
container-title: Proceedings of The 12th Asian Conference on Machine Learning
volume: '129'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 25
pdf: http://proceedings.mlr.press/v129/barakat20a/barakat20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v129/barakat20a/barakat20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
