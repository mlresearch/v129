---
title: Learning Interpretable Models using Soft Integrity Constraints
crossref: acml20
abstract: 'Integer models are of particular interest for applications where predictive
  models are supposed not only to be accurate but also interpretable to human experts.
  We introduce a novel penalty term called Facets whose primary goal is to favour
  integer weights. Our theoretical results illustrate the behaviour of the proposed
  penalty term: for small enough weights, the Facets matches the L1 penalty norm,
  and as the weights grow, it approaches the L2 regulariser. We provide the proximal
  operator associated with the proposed penalty term, so that the regularised empirical
  risk minimiser can be computed efficiently. We also introduce the Strongly Convex
  Facets, and discuss its theoretical properties. Our numerical results show that
  while achieving the state-of-the-art accuracy, optimisation of a loss function penalised
  by the proposed Facets penalty term leads to a model with a significant number of
  integer weights.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: belahcene20a
month: 0
tex_title: Learning Interpretable Models using Soft Integrity Constraints
firstpage: 529
lastpage: 544
page: 529-544
order: 529
cycles: false
bibtex_author: Belahc{\`{e}}ne, Khaled and Sokolovska, Nataliya and Chevaleyre, Yann
  and Zucker, Jean-Daniel
author:
- given: Khaled
  family: Belahc√®ne
- given: Nataliya
  family: Sokolovska
- given: Yann
  family: Chevaleyre
- given: Jean-Daniel
  family: Zucker
date: 2020-09-25
address: 
container-title: Proceedings of The 12th Asian Conference on Machine Learning
volume: '129'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 25
pdf: http://proceedings.mlr.press/v129/belahcene20a/belahcene20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
