---
title: Constrained Reinforcement Learning via Policy Splitting
crossref: acml20
abstract: We develop a model-free reinforcement learning approach to solve constrained
  Markov decision processes, where the objective and budget constraints are in the
  form of infinite-horizon discounted expectations, and the rewards and costs are
  learned sequentially from data. We propose a two-stage procedure where we first
  search over deterministic policies, followed by an aggregation with a mixture parameter
  search, that generates policies with simultaneous guarantees on near-optimality
  and feasibility. We also numerically illustrate our approach by applying it to an
  online advertising problem.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen20b
month: 0
tex_title: Constrained Reinforcement Learning via Policy Splitting
firstpage: 209
lastpage: 224
page: 209-224
order: 209
cycles: false
bibtex_author: Chen, Haoxian and Lam, Henry and Li, Fengpei and Meisami, Amirhossein
author:
- given: Haoxian
  family: Chen
- given: Henry
  family: Lam
- given: Fengpei
  family: Li
- given: Amirhossein
  family: Meisami
date: 2020-09-25
address: 
container-title: Proceedings of The 12th Asian Conference on Machine Learning
volume: '129'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 25
pdf: http://proceedings.mlr.press/v129/chen20b/chen20b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
