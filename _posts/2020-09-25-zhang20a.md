---
title: A One-step Approach to Covariate Shift Adaptation
crossref: acml20
abstract: A default assumption in many machine learning scenarios is that the training
  and test samples are drawn from the same probability distribution. However, such
  an assumption is often violated in the real world due to non-stationarity of the
  environment or bias in sample selection. In this work, we consider a prevalent setting
  called covariate shift, where the input distribution differs between the training
  and test stages while the conditional distribution of the output given the input
  remains unchanged. Most of the existing methods for covariate shift adaptation are
  two-step approaches, which first calculate the importance weights and then conduct
  importance-weighted empirical risk minimization. In this paper, we propose a novel
  one-step approach that jointly learns the predictive model and the associated weights
  in one optimization by minimizing an upper bound of the test risk. We theoretically
  analyze the proposed method and provide a generalization error bound. We also empirically
  demonstrate the effectiveness of the proposed method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang20a
month: 0
tex_title: A One-step Approach to Covariate Shift Adaptation
firstpage: 65
lastpage: 80
page: 65-80
order: 65
cycles: false
bibtex_author: Zhang, Tianyi and Yamane, Ikko and Lu, Nan and Sugiyama, Masashi
author:
- given: Tianyi
  family: Zhang
- given: Ikko
  family: Yamane
- given: Nan
  family: Lu
- given: Masashi
  family: Sugiyama
date: 2020-09-25
address: 
container-title: Proceedings of The 12th Asian Conference on Machine Learning
volume: '129'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 25
pdf: http://proceedings.mlr.press/v129/zhang20a/zhang20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
